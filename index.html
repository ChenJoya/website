<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Welcome to Joya Chen's Homepage!</title>
  
  <meta name="author" content="Joya Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="data/nus.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Joya Chen （陈卓）</name>
              </p>
              <p>I am a first-year Ph.D. student at <a href="https://sites.google.com/view/showlab">Show Lab @ NUS</a>, in <a href="https://cde.nus.edu.sg/ece/">NUS ECE</a>, under the supervision of <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl">Prof. Mike Shou</a>. Before that I was a research assistant there, where I focused on computer vision research, especially in multimodal video understanding. I also had a research assistant at <a href="https://sites.google.com/comp.nus.edu.sg/cvml">CVML@NUS group</a> from Aug. 2021 to Jan. 2022.
              </p>
              <p>
                I obtained my bachelor's degree in <a href="http://auto.whut.edu.cn/">School of Automotive Engineering, WUT</a> in Jun. 2018. To chase my AI dream, I took the National Postgraduate Entrance Examination and obtained the 1st place in <a href="https://en.cs.ustc.edu.cn/">School of Computer Science and Technology, USTC</a>, in Mar. 2018. I obtained my master's degree from here in Jun. 2021, under the supervision of <a href="http://staff.ustc.edu.cn/~cheneh/">Prof. Enhong Chen</a>, <a href="http://staff.ustc.edu.cn/~tongxu/">Prof. Tong Xu</a>, and <a href="http://staff.ustc.edu.cn/~dongeliu/">Prof. Dong Liu</a>. 
              </p>
              <p>Nice to meet you: joyachen97@gmail.com :)
              </p>
              <p style="text-align:center">
                <a href="data/joyachen.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=IIx9dc8AAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/chenjoya/">Github</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/chenjoya">Zhihu</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:40%">
              <a href="data/joyachen.png"><img style="width:100%;max-width:100%" alt="profile photo" src="data/joyachen.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests lie in multimodal video understanding.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/assistq.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2203.04203">
                <papertitle>AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant</papertitle>
              </a>
              <br>
							Benita Wong*, <strong>Joya Chen*</strong>, You Wu*, Stan Weixian Lei, Dongxing Mao, Difei Gao, Mike Zheng Shou (*Equal contribution)
              <br>
              <em>Accepted by ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.04203">Paper</a>
              /
              <a href="https://showlab.github.io/assistq">Page</a>
              /
              <a href="https://github.com/chenjoya/q2a">Code</a>
	      /
              <a href="https://sites.google.com/view/loveucvpr22">Challenge@CVPR'22</a>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/dropit.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2202.13808">
                <papertitle>DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training</papertitle>
              </a>
              <br>
							<strong>Joya Chen*</strong>, Kai Xu*, Yifei Cheng, Angela Yao (*Equal contribution)
              <br>
              <em>arXiv Preprint</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2202.13808">Paper</a>
              /
              <a href="https://github.com/chenjoya/dropit">Code</a>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/sampling_free.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://ieeexplore.ieee.org/document/9526287">
                <papertitle>Is Heuristic Sampling Necessary in Training Deep Object Detectors? </papertitle>
              </a>
              <br>
							<strong>Joya Chen</strong>, Dong Liu, Tong Xu, Shiwei Wu, Yifei Chen, Enhong Chen
              <br>
              <em>IEEE Transactions on Image Processing</em>, 2021 
              <br>
              <a href="https://ieeexplore.ieee.org/document/9526287">Paper</a>
              /
              <a href="https://github.com/ChenJoya/sampling-free">Code</a>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/sogg.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://dl.acm.org/doi/10.1145/3474085.3475684">
              <papertitle>Linking the Characters: Video-oriented Social Graph Generation via Hierarchical-cumulative GCN</papertitle>
              </a>
              <br>
              Shiwei Wu, <strong>Joya Chen</strong>, Tong Xu, Liyi Chen, Lingfei Wu, Yao Hu, Enhong Chen
              <br>
              <em>ACM MM</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3474085.3475684">Paper</a>
            </td>
          
            <tr>
              <td style="padding:20px;width:25%;vertical-align:top"><img src='data/resobj.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/1908.09075">
                <papertitle>Residual Objectness for Imbalance Reduction</papertitle>
              </a>
              <br>
              <strong>Joya Chen</strong>, Dong Liu, Bin Luo, Xuezheng Peng, Tong Xu, Enhong Chen. 
              <br>
              <em>Pattern Recognition</em>, 2022 
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S003132032200262X">Paper</a> /
              <a href="https://github.com/ChenJoya/resobj">Code</a> 
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/manogcn.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://ieeexplore.ieee.org/abstract/document/9428299">
              <papertitle>Capturing Implicit Spatial Cues for Monocular 3D Hand Reconstruction</papertitle>
              </a>
              <br>
              Qi Wu*, <strong>Joya Chen*</strong>, Zhou Xu, ZhiMing Yao, Xianjun Yang (*Equal contribution)
              <br>
              <em>IEEE ICME</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9428299">Paper</a>
              /
              <a href="https://github.com/ChenJoya/manogcn">Code</a>
            </td>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:top"><img src='data/crossgraphalign.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://www.sciengine.com/doi/10.1360/SSI-2019-0292">
                <papertitle>Cross-Modal Video Moment Retrieval Based on Visual-Textual Relationship Alignment</papertitle>
              </a>
              <br>
              <strong>Joya Chen</strong>, Hao Du, YufeiWu, Tong Xu, Enhong Chen.
              <br>
              <em>SCIENTIA SINICA Informationis</em>, 2020 (in Chinese)
              <br>
              <a href="https://www.sciengine.com/doi/10.1360/SSI-2019-0292">Paper</a> 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/fgbg.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://ieeexplore.ieee.org/document/9175512">
                <papertitle>Foreground-Background Imbalance Problem in Deep Object Detectors: A Review</papertitle>
              </a>
              <br>
              <strong>Joya Chen</strong>, QiWu, Dong Liu, Tong Xu
              <br>
              <em>IEEE MIPR</em>, 2020
              <br>
              <a href="https://ieeexplore.ieee.org/document/9175512">Paper</a> 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/ovsampler.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://ieeexplore.ieee.org/abstract/document/9093499">
                <papertitle>Overlap Sampler for Region-Based Object Detection</papertitle>
              </a>
              <br>
              <strong>Joya Chen</strong>, Bin Luo, Qi Wu, Jia Chen, Xuezheng Peng
              <br>
              <em>IEEE WACV</em>, 2020
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9093499">Paper</a> /
              <a href="https://github.com/ChenJoya/overlap-sampler">Code</a> 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/jlrls.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/1910.12283">
                <papertitle>Long-term Joint Scheduling for Urban Traffic</papertitle>
              </a>
              <br>
              Xianfeng Liang, Likang Wu, <strong>Joya Chen</strong>, Yang Liu, Runlong Yu, Min Hou, Han Wu, Yuyang Ye, Qi Liu, Enhong Chen
              <br>
              <em>KDD CUP</em>, 2019 &nbsp <font color="red"><strong>(PaddlePaddle Special Award)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1910.12283">Paper</a> /
              <a href="https://github.com/bigdata-ustc/Long-term-Joint-Scheduling">Code</a> 
            </td>
          </tr> 
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Engineering</heading>
            </td>
          </tr>
        </tbody></table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/ho3d.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              Ranked 1st in <a href="https://competitions.codalab.org/competitions/22485#results">HO-3D Leaderboard</a> in Mesh Error/AUC and F@15mm metrics in Dec. 2020</a>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/pascal2.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              Ranked 1st in <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_main.php?challengeid=11&compid=3">PASCAL VOC Object Detection Competition 3 Leaderboard</a> in Sep. 2018</a>
            </td>
          </tr>
        </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Work</heading>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:top"><img src='data/tencent.jpg' width="160"></td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                Worked as a research intern in <a href="https://www.tencent.com/en-us">Tencent</a> from Jun. 2018 to Nov. 2019
              </td>
            </tr> 
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thanks go to <a href="https://jonbarron.info/">Jon Barron's website</a>!
                </p>
              </td>
            </tr>
          </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
