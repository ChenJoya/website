<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Welcome to Joya Chen's Homepage!</title>
  
  <meta name="author" content="Joya Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="data/nus.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Joya Chen （陈卓）</name>
              </p>
	      
	      <p>
		      Hi! This is Joya, a second-year Ph.D. student at <a href="https://sites.google.com/view/showlab">Show Lab @ NUS</a>, supervised by <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl">Prof. Mike Shou</a>. Currently, I'm interning at FAIR, Meta AI, working with <a href="https://csrhddlam.github.io/">Huiyu Wang</a>. Previously I also worked with <a href="https://lvzhaoyang.github.io/">Zhaoyang Lv</a> from Reality Labs Research, Meta. My research interest is in large multimodal models, particularly learning them for real-time, streaming video. 
	      </p>
	      <details>
	      <summary>View my education background</summary>
	      <p>
		      I obtained my bachelor's degree in <a href="http://auto.whut.edu.cn/">School of Automotive Engineering, WUT</a>. To chase my AI dream, I took the National Postgraduate Entrance Examination and obtained the 1st place in <a href="https://en.cs.ustc.edu.cn/">School of Computer Science and Technology, USTC</a>. I obtained my master's degree from here, under the supervision of <a href="http://staff.ustc.edu.cn/~cheneh/">Prof. Enhong Chen</a>, <a href="http://staff.ustc.edu.cn/~tongxu/">Prof. Tong Xu</a>, and <a href="http://staff.ustc.edu.cn/~dongeliu/">Prof. Dong Liu</a>. I also had a research assistant at <a href="https://sites.google.com/comp.nus.edu.sg/cvml">CVML@NUS group</a>, working closely with <a href="https://www.comp.nus.edu.sg/~ayao/">Prof. Angela Yao</a>.  
	      </p>		
	      </details>
              <p>Nice to meet you: joyachen@u.nus.edu :)</p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=IIx9dc8AAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/chenjoya/">Github</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/chenjoya">Zhihu</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:40%">
              <a href="data/joyachen.png"><img style="width:100%;max-width:100%" alt="profile photo" src="data/joyachen.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
	</tbody></table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/videollm-online.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://showlab.github.io/videollm-online/">
                <papertitle>VideoLLM-online: Online Video Large Language Model for Streaming Video</papertitle>
              </a>
              <br>
		<strong>Joya Chen</strong>, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Zheng Shou
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://showlab.github.io/videollm-online/">Homepage: Paper, Code, Data, Demo, Checkpoints</a>
            </td>
          </tr>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/egoexo4d.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://ego-exo4d-data.org/">
                <papertitle>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</papertitle>
              </a>
              <br>
		Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, <strong>Joya Chen</strong>, ..., Mike Zheng Shou, Michael Wray
              <br>
              <em>CVPR (Oral)</em>, 2024
              <br>
              <a href="https://ego-exo4d-data.org/">https://ego-exo4d-data.org/</a>
            </td>
          </tr>
		      
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/assistgpt.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2306.08640">
                <papertitle>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</papertitle>
              </a>
              <br>
		Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, <strong>Joya Chen</strong>, Zihan Fan, Mike Zheng Shou
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2306.08640.pdf">Paper</a>
              /
              <a href="https://showlab.github.io/assistgpt/">Page</a>
            </td>
          </tr> 
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/univtg.jpg' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2307.16715">
                <papertitle>UniVTG: Towards Unified Video-Language Temporal Grounding</papertitle>
              </a>
              <br>
		Kevin Qinghong Lin, Pengchuan Zhang, <strong>Joya Chen</strong>, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, Mike Zheng Shou
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2307.16715.pdf">Paper</a>
              /
              <a href="https://github.com/showlab/UniVTG">Code</a>
	      /
              <a href="https://huggingface.co/spaces/KevinQHLin/UniVTG">Demo</a>
            </td>
          </tr> 
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/afformer.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2303.14644">
                <papertitle>Affordance Grounding from Demonstration Video to Target Image</papertitle>
              </a>
              <br>
							<strong>Joya Chen</strong>, Difei Gao, Kevin Qinghong Lin, Mike Zheng Shou
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.14644.pdf">Paper</a>
              /
              <a href="https://github.com/showlab/afformer">Code</a>
            </td>
          </tr> 
		
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/dropit.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2202.13808">
                <papertitle>DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training</papertitle>
              </a>
              <br>
							<strong>Joya Chen*</strong>, Kai Xu*, Yuhui Wang, Yifei Cheng, Angela Yao (*Equal contribution)
              <br>
              <em>ICLR</em>, 2023
              <br>
              <a href="https://openreview.net/forum?id=Kn6i2BZW69w">OpenReview</a>
              /
	      <a href="https://arxiv.org/abs/2202.13808">arXiv</a>
              /
              <a href="https://github.com/chenjoya/dropit">Code</a>
            </td>
          </tr> 
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/assistq.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2203.04203">
                <papertitle>AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant</papertitle>
              </a>
              <br>
							Benita Wong*, <strong>Joya Chen*</strong>, You Wu*, Stan Weixian Lei, Dongxing Mao, Difei Gao, Mike Zheng Shou (*Equal contribution)
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.04203">Paper</a>
              /
              <a href="https://showlab.github.io/assistq">Page</a>
              /
              <a href="https://github.com/chenjoya/q2a">Code</a>
	      /
              <a href="https://sites.google.com/view/loveucvpr22">Challenge@CVPR'22</a>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/sampling_free.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://ieeexplore.ieee.org/document/9526287">
                <papertitle>Is Heuristic Sampling Necessary in Training Deep Object Detectors? </papertitle>
              </a>
              <br>
							<strong>Joya Chen</strong>, Dong Liu, Tong Xu, Shiwei Wu, Yifei Chen, Enhong Chen
              <br>
              <em>IEEE Transactions on Image Processing</em>, 2021 
              <br>
              <a href="https://ieeexplore.ieee.org/document/9526287">Paper</a>
              /
              <a href="https://github.com/ChenJoya/sampling-free">Code</a>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/sogg.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://dl.acm.org/doi/10.1145/3474085.3475684">
              <papertitle>Linking the Characters: Video-oriented Social Graph Generation via Hierarchical-cumulative GCN</papertitle>
              </a>
              <br>
              Shiwei Wu, <strong>Joya Chen</strong>, Tong Xu, Liyi Chen, Lingfei Wu, Yao Hu, Enhong Chen
              <br>
              <em>ACM MM (Oral)</em>, 2021
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3474085.3475684">Paper</a>
            </td>
          </tr> 

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Engineering</heading>
            </td>
          </tr>
        </tbody></table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/ho3d.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              Ranked 1st in <a href="https://competitions.codalab.org/competitions/22485#results">HO-3D Leaderboard</a> in Mesh Error/AUC and F@15mm metrics in Dec. 2020</a>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top"><img src='data/pascal2.png' width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              Ranked 1st in <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_main.php?challengeid=11&compid=3">PASCAL VOC Object Detection Competition 3 Leaderboard</a> in Sep. 2018</a>
            </td>
          </tr>
        </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Intern</heading>
              </td>
            </tr>
          </tbody></table>

	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:top"><img src='data/meta.png' width="160"></td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                Worked as a AI research scientist intern at <a href="https://ai.meta.com/research/">FAIR, Meta AI</a> from Dec. 2023 to May. 2024
              </td>
            </tr> 
          </tbody></table>
	      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:top"><img src='data/tencent.jpg' width="160"></td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                Worked as a computer vision research intern in <a href="https://www.tencent.com/en-us">Tencent</a> from Jun. 2018 to Nov. 2019
              </td>
            </tr> 
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thanks go to <a href="https://jonbarron.info/">Jon Barron's website</a>!
                </p>
              </td>
            </tr>
          </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
